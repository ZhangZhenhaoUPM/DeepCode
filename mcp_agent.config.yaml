$schema: ./schema/mcp-agent.config.schema.json
anthropic: null
default_search_server: brave
document_segmentation:
  enabled: true
  size_threshold_chars: 3000
execution_engine: asyncio
logger:
  level: debug  # Changed from 'info' to 'debug' for detailed troubleshooting
  path_settings:
    path_pattern: logs/mcp-agent-{unique_id}.jsonl
    timestamp_format: '%Y%m%d_%H%M%S'
    unique_id: timestamp
  progress_display: true
  transports:
  - console
  - file
mcp:
  servers:
    bocha-mcp:
      args:
      - tools/bocha_search_server.py
      command: python3
      env:
        BOCHA_API_KEY: ''
        PYTHONPATH: .
    brave:
      args:
      - -y
      - '@modelcontextprotocol/server-brave-search'
      command: npx
      env:
        BRAVE_API_KEY: 'BSA5zcmimzOanUyFjc2xlckqHYkBgTp'
    code-implementation:
      args:
      - tools/code_implementation_server.py
      command: python
      description: Paper code reproduction tool server - provides file operations,
        code execution, search and other functions
      env:
        PYTHONPATH: .
    code-reference-indexer:
      args:
      - tools/code_reference_indexer.py
      command: python
      description: Code reference indexer server - Provides intelligent code reference
        search from indexed repositories
      env:
        PYTHONPATH: .
    command-executor:
      args:
      - tools/command_executor.py
      command: python
      env:
        PYTHONPATH: .
    document-segmentation:
      args:
      - tools/document_segmentation_server.py
      command: python
      description: Document segmentation server - Provides intelligent document analysis
        and segmented reading to optimize token usage
      env:
        PYTHONPATH: .
    fetch:
      args:
      - mcp-server-fetch
      command: uvx
    file-downloader:
      args:
      - tools/pdf_downloader.py
      command: python
      env:
        PYTHONPATH: .
    filesystem:
      args:
      - -y
      - '@modelcontextprotocol/server-filesystem'
      - .
      command: npx
    github-downloader:
      args:
      - tools/git_command.py
      command: python
      env:
        PYTHONPATH: .
openai:
  base_max_tokens: 20000
  default_model: gemini-2.5-pro
  max_tokens_policy: adaptive
  retry_max_tokens: 32768
ollama:
  base_max_tokens: 65535  # Maximum tokens for code generation (2^16 - 1)
  default_model: qwen3:32b  # General model for analysis and planning
  code_model: qwen3-coder:30b  # Specialized model for code generation
  vision_model: qwen3-vl:4b  # Vision model for image understanding
  max_tokens_policy: adaptive
  retry_max_tokens: 65535  # Maximum tokens for retry attempts
  # Model-specific generation parameters (following Qwen3 official best practices)
  generation_params:
    qwen3:32b:  # For thinking/reasoning mode (analysis tasks)
      temperature: 0.6  # Qwen3 doc: use 0.6 for thinking mode, avoid greedy decoding
      top_p: 0.95
      top_k: 20
      min_p: 0.0
      repeat_penalty: 1.05
    qwen3-coder:30b:  # For code generation (official recommendations)
      temperature: 0.7  # Official Qwen3-Coder recommendation
      top_p: 0.8       # Official recommendation
      top_k: 20        # Official recommendation
      min_p: 0.0
      repeat_penalty: 1.05  # Official recommendation
    qwen3-vl:4b:  # For vision tasks
      temperature: 0.3
      top_p: 0.9
      top_k: 20
      min_p: 0.0
      repeat_penalty: 1.05
default_llm_provider: ollama
vision_enabled: true  # Enable vision/image understanding with VLM
planning_mode: traditional
# Multi-model strategy: use appropriate model for each task type
task_model_routing:
  enabled: true
  strategies:
    analysis: qwen3:32b  # Text analysis, planning, understanding (general purpose)
    code_generation: qwen3-coder:30b  # Code writing, implementation (code specialized)
    vision: qwen3-vl:4b  # Image understanding, diagram analysis
